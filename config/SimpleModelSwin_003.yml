# use drop out layer
# change epoch and min learning rate

global:
  debug: false
  seed: 42
  device: cuda

amp: true

train:
  max_epoch: 20
  fold: 5
  shuffle: true

augmentation:
  train:
    Resize: { height: 224, width: 224 }
    Normalize: {mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225]}
    ToTensor: {always_apply: True}
  valid:
    Resize: {height: 224, width: 224}
    Normalize: {mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225]}
    ToTensor: {always_apply: True}

dataset:
  name: TrainDataset

loader:
  train: {batch_size: 32, shuffle: True, num_workers: 2, pin_memory: True, drop_last: True}
  valid: {batch_size: 256, shuffle: False, num_workers: 2, pin_memory: True, drop_last: False}

model:
  name: SimpleModelDrop
  params:
    base_model: swin_tiny_patch4_window7_224
    pretrained: true
    in_channels: 3
    fc_dim: 1
    dropout: 0.5

optimizer:
  name: Adam
  params:
    lr: 1.0e-3

loss: MSELoss

scheduler:
  name: CosineAnnealingWarmRestarts
  params:
    T_0: 20
    T_mult: 1
    eta_min: 1.0e-8

accumulation: 2

metrics:
  - mean_squared_error

